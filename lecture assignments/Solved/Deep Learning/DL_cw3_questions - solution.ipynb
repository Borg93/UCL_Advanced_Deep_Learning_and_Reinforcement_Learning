{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_cw3_questions - solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoritzTaylor/UCL_Advanced_Deep_Learning_and_Reinforcement_Learning/blob/master/lecture%20assignments/Solved/Deep%20Learning/DL_cw3_questions%20-%20solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GHl6N3Xfkctm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Homework 3\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "\n",
        "**Name:** Your Name\n",
        "\n",
        "**SN:** Your Student Number\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Start date:** *12th Feb 2018*\n",
        "\n",
        "**Due date:** *5th March 2018, 11:55 pm*\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_DL_hw3.ipynb** before the deadline above.\n",
        "\n",
        "Also send a **sharable link** to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.\n",
        "\n",
        "Please compile all results ( table in Q2) and all answers to the understanding/analysis results questions (Q1 and Q3), into a PDF. Name convention: **studentnumber_DL_hw3.pdf**. Do not include any of the code (we will use the notebook for that). \n",
        "\n",
        "**Page limit: 7 pg ** (w/o the bonus question).\n",
        "\n",
        "------------------------------------------------\n",
        "\n",
        "## MNIST as a sequence\n",
        "In this assignment we will be using the [MNIST digit dataset](https://yann.lecun.com/exdb/mnist/). The dataset contains images of hand-written digits ($0-9$), and the corresponding labels. The images have a resolution of $28\\times 28$ pixels. This is the same dataset as in Assignment 1, but we will be using this data a bit differently this time around. Since this assignment will be focusing on recurrent networks that model sequential data, we will be looking at each image as a sequence: the networks you train will be \"reading\" the image one row at a time, from top to bottom (we could even do pixel-by-pixel, but in the interest of time we'll do row-by-row which is faster).  Also, we will work with a binarized version of MNIST -- we constrain the values of the pixels to be either $0$ or $1$. You can do this by applying the method `binarize`, defined below, to the raw images.\n",
        "\n",
        "<img src=\"https://github.com/bodono/files/blob/master/mnist_as_sequence.png?raw=true\">\n",
        "\n",
        "* We take the MNIST images, binarise them, and interpret them as a sequence of pixels from top-left to bottom-right. (\"Task 2\" refers to the next homework, wherein you will be using the sequence for pixel prediction).\n",
        "\n",
        "## Recurrent Models for MNIST\n",
        "\n",
        "As discussed in the lectures, there are various ways and tasks for which we can use recurrent models. A depiction of the most common scenarios is available in the Figure below. In this assignment and the following one we will look at two of these forms: **many-to-one** (sequence to label/decision) and the **many-to-many** scenario where the model receives an input and produces an output at every time step. You will use these to solve the following tasks: i) classification (*this homework*), ii) pixel prediction (*next homework*) and iii) in-painting (*next homework*).\n",
        "\n",
        "<img src=\"https://github.com/bodono/files/blob/master/rnn_models.png?raw=true\">\n",
        "* ([Figure adapted from Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness)). You will be implementing variants of *many-to-one* for classification (in this homework), and *many-to-many* for prediction (in the next homework).\n"
      ]
    },
    {
      "metadata": {
        "id": "Dj6WmOJKYRTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1: Understanding LSTM vs GRU (30 pts)\n",
        "Before going deeper into your practical tasks, take some time to revise and make sure you understand the two major types of recurrent cells you will be using in this assignment: Long-Short Term Memory Units (LSTM) first introduced by Hochreiter and Schmidhuber [1997] and the more recent Gated Recurrent Units (GRU) by Cho et al. [2014]. Once you have done this, answer the following questions:\n",
        "\n",
        "1. Can LSTMs (and respectively GRUs) just store the current input in the state ($c_t$ for LSTM and $h_t$ for GRU, in the class notation) for the next step? If so, give the gates activation that would enable this behaviour. If not, explain why not. [10 pts]\n",
        "2. Can LSTMs (and respectively GRUs) just store a previous state into the current state and ignore the current input? If so, give the gates' activation that would enable this. If not, explain why not. [10 pts]\n",
        "3. Are GRUs a special case of LSTMs? If so, give the expression of the GRU gates in term of LSTM's gates ($o_t, i_t, f_t$). If not, give a counter-example. Assume here the same input. [10 pts]\n",
        "\n",
        "**Word limit: ** 1000 words or less\n",
        "\n",
        "**Answers:**\n",
        "1. No, it would not be possible. To realise this, the forget gate $f_t$ should output 0 for every number in the cell state $c_{t-1}$, while the input gate $i_t$ should output 1 for every number in the cell state $c_{t-1}$. Since the sigmoid function of the gates, can only be 0 or 1 in infinity, it is practically impossible. Furthermore to use only the current input, we have to zero-out $h_{t-1}$, so that it does not get incorporated into the input gate. This means that the weight matrix $W_{ih}$ of $h_{t-1}$ has to be zero for every value of $h_{t-1}$. In a GRU, we have a similar situation.\n",
        "\n",
        "2. To realise this behaviour in a LSTM, the input gate $i_t$ should ouput 0 for every number in the cell state $c_{t-1}$, while the forget gate $f_t$ should output 1 for every number in the cell state $c_{t-1}$. But again, the sigmoid function will never be 0 or 1. Nevertheless this scenario would be possible, if $j_t$ would be 0. Then only the last cell state $c_{t-1}$ will be incorporated into $c_t$. In a GRU, the output $z_t$ should be 1 for every number in the cell state $h_{t-1}$. Then $\\tilde{h}$ will be ingnored completly. Since $z_t$ is calculated with a sigmoid function again, this scenario will not be possible practically.\n",
        "\n",
        "3. GRU combines the forget and input gates into a single “update gate.” It also merges the cell state $c_t$ and hidden state $h_t$, and makes some other changes. The resulting model is simpler than a standard LSTM model. ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cqrTk9bLYTpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2: Implementation. Line-by-Line MNIST Classification (50 pts)\n",
        "In this part you will train a number of many-to-one recurrent models that takes as input: an image (or part of an image) as a sequence (row by row) and after the last input row produces, as output, a probability distribution over the $10$ possible labels ($0-9$). The models will be trained using a cross-entropy loss function over these output probabilities.\n",
        "\n",
        "### Optimization\n",
        "Use the Adam optimizer (with default settings other than the learning rate) for training.\n",
        "\n",
        "**[Optional]** Sometimes dropout has been shown to be beneficial in training recurrent models, so feel free to use it or any other form of regularization that seems to improve performance. It might be also worth trying out batch-normalization. [Reference](https://arxiv.org/pdf/1603.09025.pdf).\n",
        "\n",
        "### Models: Your models will have the following structure:\n",
        "1. [(Red Block)] The *input* (current binarised row of pixels) can be fed directly into the recurrent connection without  much further pre-processing. The only thing you need to do is have an affine transformation to match the dimensionality of the recurrent unit, i.e. one of $(32, 64, 128)$.\n",
        "2. [(Blue Block)] The *output* (probabilities over the 10 classes) is produced by looking at the last output of the recurrent units, transforming them via an affine transformation.\n",
        "3. [(Green Block)] For the *recurrent* part of the network, please implement and compare the following architectures:\n",
        "    * LSTM with 32, 64, 128 units. [15 pts]\n",
        "    * GRU with 32, 64, 128 units. [15 pts]\n",
        "    * stacked LSTM: 3 recurrent layers with 32 units each. [10 pts]\n",
        "    * stacked GRU: 3 recurrent layers with 32 units each. [10 pts]\n",
        "\n",
        "Your network should look like:\n",
        "\\begin{equation}\n",
        "\\textrm{Input} \\Rightarrow \\textrm{RNN cell} \\Rightarrow \\textrm{Relu} \\Rightarrow \\textrm{Fully connected} \\Rightarrow \\textrm{Relu} \\Rightarrow \\textrm{Fully connected} \\Rightarrow \\textrm{Output}\n",
        "\\end{equation}\n",
        "You might find the function `tf.nn.dynamic_rnn` useful.\n",
        "\n",
        "### Hyper-parameters \n",
        "For all cases train the model with these hyper-parameter settings:\n",
        "\n",
        "- *num_epochs*=10, *learning_rate*=0.001, *batch_size*=256, *fully_connected_hidden_units=64*\n",
        "\n",
        "With these hyper-parameters you should be comfortably above $95\\%$ test set accuracy on all tasks. (Feel free to try other settings, there are certainly better choices, but please report the results with these exact hyper-parameters). Please report the *cross-entropy* and the *classification accuracy* for the *test set* of the models trained. Use the `plot_summary_table` method below to format the table.\n"
      ]
    },
    {
      "metadata": {
        "id": "3oMPARSkfTO9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Q3: Analyse the results (20 pts + 10 pts)\n",
        "\n",
        "1. How does this compare with the results you obtained in the first assignment(DL1), when training a model that \"sees\" the entire image at once? Explain differences. [5 pts]\n",
        "2. Let us take a look closer look at one of the trained models: say GRU (32). Plot the outputs of the RNN layer and hidden state over time. In particular, look at the first 3-5 time steps. Plot the input image along side. \n",
        "You can use ```python\n",
        "plt.imshow(output_GRU_over_time)\n",
        "```\n",
        "for these, where ```output_GRU_over_time.shape``` is (T=28,hidden_units) dimensional. What do you observe? Show at least one pair of these plots to support your observation(s). [5 pts]\n",
        "3. Now, look at the last 3-5 time steps. What do you observe? When is the classification decision made? To validate your answer to the second part of the question, provide the classification predictions for the last 5 time steps -- that is, pretend whatever the output of the GRU is at that time step is in fact the last output in the computation, and feed that into the classification mapping. [10 pts] \n",
        "\n",
        "**Word limit:** 500 words or less \n",
        "\n",
        "\n",
        "**[Bonus]** Let's looking inside the computation. Take one of the previous LSTM models (for simplicity pick one of the one-layer recurrence models) and track the status of the gates ($o_t, i_t, f_t$) over time. Note that if you used the provided RNNCell wrappers (BasicLSTMCell and co) in tensorflow, these will keep this information hidden, so you will need to implement your own version of this recurrence layer, or mirror the one in tensorflow, but now exposing these hidden variables. A bit of warning: this is not trivial and will require some thinking on the coding side, but it will also provide you with a more informative way of visulazing the inner computation.  [10 pts]\n",
        "\n",
        "**Word limit:** 300 words or less "
      ]
    },
    {
      "metadata": {
        "id": "0wg-OYIMm8cW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports and utility functions (do not modify!)"
      ]
    },
    {
      "metadata": {
        "id": "rPYnfzQfltxp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import useful libraries.\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "\n",
        "# Binarize the images\n",
        "def binarize(images, threshold=0.1):\n",
        "  return (threshold < images).astype('float32')\n",
        "\n",
        "# Import dataset with one-hot encoding of the class labels.\n",
        "def get_data():\n",
        "  return input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# Placeholders to feed train and test data into the graph.\n",
        "# Since batch dimension is 'None', we can reuse them both for train and eval.\n",
        "def get_placeholders():\n",
        "  x = tf.placeholder(tf.float32, [None, 784])\n",
        "  y = tf.placeholder(tf.float32, [None, 10])\n",
        "  return x, y\n",
        "\n",
        "# Generate summary table of results. This function expects a dict with the\n",
        "# following structure: keys of 'LSTM' or 'GRU' and the values for each key are a\n",
        "# list of tuples consisting of (test_loss, test_accuracy), and the list is\n",
        "# ordered as the results from 32 units, 64 units, 128 units, 3 x 32 units, i.e.:\n",
        "# {\n",
        "#  'LSTM': [(loss,acc), (loss, acc), (loss, acc), (loss, acc)]\n",
        "#  'GRU': [(loss,acc), (loss, acc), (loss, acc), (loss, acc)]\n",
        "# }\n",
        "def plot_summary_table(experiment_results):\n",
        "  # Fill Data.\n",
        "  cell_text = []\n",
        "  columns = ['(1 layer, 32 units)', '(1 layer, 64 units)', '(1 layer, 128 units)', '(3 layers, 32 units)']\n",
        "  for k, v in experiment_results.iteritems():\n",
        "    rows = ['Test loss', 'Test accuracy']\n",
        "    cell_text=[[],[]]\n",
        "    for (l, _) in v:\n",
        "      cell_text[0].append(str(l))\n",
        "    for (_, a) in v:\n",
        "      cell_text[1].append(str(a))\n",
        "\n",
        "    fig=plt.figure(frameon=False)\n",
        "    ax = plt.gca()\n",
        "    the_table = ax.table(\n",
        "      cellText=cell_text,\n",
        "      rowLabels=rows,\n",
        "      colLabels=columns,\n",
        "      loc='center')\n",
        "    the_table.scale(2, 8)\n",
        "    # Prettify.\n",
        "    ax.patch.set_facecolor('None')\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.text(-0.73, 0.9, k, fontsize=18)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6ecVcTO3xZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Models\n",
        "\n",
        "Generate summary table of results. This function expects a dict with the\n",
        "following structure: keys of 'LSTM' or 'GRU' and the values for each key are a\n",
        "list of tuples consisting of (test_loss, test_accuracy), and the list is\n",
        "ordered as the results from 32 units, 64 units, 128 units, 3 x 32 units, i.e. expected dictionary (final performace only):\n",
        "\n",
        "```python\n",
        "{\n",
        "  'LSTM': [(loss,acc), (loss, acc), (loss, acc), (loss, acc)]\n",
        "  'GRU': [(loss,acc), (loss, acc), (loss, acc), (loss, acc)]\n",
        "}\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "LyZNQvH26R1q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def FullyConnectedPart(x):\n",
        "  # ReLU with the output of the RNN-Cell given as x\n",
        "  y_1 = tf.nn.relu(x)\n",
        "  \n",
        "  # Linear activation + ReLU\n",
        "  y_2 = tf.nn.relu( tf.matmul(y_1, W_1) + b_1 )\n",
        "  \n",
        "  y_logits = tf.matmul(y_2, W_2) + b_2\n",
        "  return tf.nn.softmax( y_logits ), y_logits\n",
        "\n",
        "\n",
        "def LSTM(x):\n",
        "  # Prepare data shape to match `lstm` function requirements\n",
        "  # Current data input shape: (batch_size, timesteps*n_input)\n",
        "  # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
        "  \n",
        "  # Reshape to get a tensor with shape: (batch_size, timesteps, n_input)\n",
        "  x = tf.reshape(x, [timesteps, num_input])\n",
        "  \n",
        "  # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
        "  x = tf.unstack(x, timesteps, 1)\n",
        "  \n",
        "  # Define a lstm cell with tensorflow\n",
        "  lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden_RNN)\n",
        "  \n",
        "  # Get lstm cell output\n",
        "  outputs, states = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "  \n",
        "  # Returning rnn inner loop last output\n",
        "  return outputs[-1]\n",
        "\n",
        "  \n",
        "def GRU(x):\n",
        "  \n",
        "  # Prepare data shape to match `rnn` function requirements\n",
        "  # Current data input shape: (batch_size, timesteps, n_input)\n",
        "  # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
        "  \n",
        "  # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
        "  x = tf.unstack(x, timesteps, 1)\n",
        "  \n",
        "  # Define a lstm cell with tensorflow\n",
        "  gru_cell = rnn.GRUCell(num_hidden, forget_bias=1.0)\n",
        "  \n",
        "  # Get lstm cell output\n",
        "  outputs, states = rnn.dynamic_rnn(gru_cell, x, dtype=tf.float32)\n",
        "  \n",
        "  # Linear activation, using rnn inner loop last output\n",
        "  return outputs[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7gQY9NP04tQz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your training session here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHumALORa0tW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 256\n",
        "num_epochs = 10\n",
        "\n",
        "# Using Xavier Init\n",
        "initializer = tf.contrib.layers.xavier_initializer() \n",
        "\n",
        "# Network Parameters\n",
        "num_input = 28 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 # timesteps\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "fully_connected_hidden_units = 64 # Num of hidden units in the fc layers\n",
        "\n",
        "experiment_results = {\n",
        "    \"LSTM\": [],\n",
        "    \"GRU\": []   \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGLCW-RPgjFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "13b8d88d-4235-4324-fc2a-96900861a12b"
      },
      "cell_type": "code",
      "source": [
        "# LSTM with 32 units\n",
        "num_hidden_RNN = 32 # rnn hidden layer num of features\n",
        "\n",
        "# tf Graph input\n",
        "X, Y = get_placeholders()\n",
        "\n",
        "# Init weight and bias for the fc layers\n",
        "W_1 = tf.Variable(initializer( [num_hidden_RNN, fully_connected_hidden_units]  ))\n",
        "b_1 = tf.Variable(initializer( [fully_connected_hidden_units] ) )\n",
        "\n",
        "W_2 = tf.Variable(initializer( [fully_connected_hidden_units, num_classes]  ))\n",
        "b_2 = tf.Variable(initializer( [num_classes] ) )\n",
        "\n",
        "lstm_output = LSTM(X)\n",
        "prediction, logits = FullyConnectedPart(lstm_output)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model (with test logits, for dropout to be disabled)\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  while mnist.train.epochs_completed < num_epochs:\n",
        "    \n",
        "    # Get batch from training data\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    \n",
        "    # Reshape data to get 28 seq of 28 elements\n",
        "    batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "    batch_x = binarize(batch_x)\n",
        "    \n",
        "    # Run optimization op (backprop)\n",
        "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "    \n",
        "# Calc loss and accuracy\n",
        "test_data = mnist.test.images\n",
        "test_label = mnist.test.labels\n",
        "loss, acc = sess.run([loss_op, accuracy], feed_dict={X: test_data, Y: test_label})\n",
        "\n",
        "experiment_results[\"LSTM\"].append( (loss, acc) )"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-95031ae28473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mb_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlstm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnectedPart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-91dbe587ca18>\u001b[0m in \u001b[0;36mLSTM\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# Get lstm cell output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# Returning rnn inner loop last output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    625\u001b[0m           sequence_length, name=\"sequence_length\")\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_best_effort_input_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_best_effort_input_batch_size\u001b[0;34m(flat_input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       raise ValueError(\n\u001b[0;32m---> 91\u001b[0;31m           \"Expected input tensor %s to have rank at least 2\" % input_)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input tensor Tensor(\"unstack_4:0\", shape=(28,), dtype=float32) to have rank at least 2"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hr-L9cZl37ek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Results"
      ]
    },
    {
      "metadata": {
        "id": "CtDksStF35Vb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_summary_table(experiment_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3CNBAL086a6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis of results"
      ]
    },
    {
      "metadata": {
        "id": "xATSlG0x6YpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Look into the GRU computation over time\n",
        "\n",
        "# plot one sample over time\n",
        "plt.imshow(output_GRU_over_time)\n",
        "plt.imshow(binarized_input_image)\n",
        "\n",
        "# Feel free to pick a sample from the dataset that more closely supports your \n",
        "# answers in Q3.\n",
        "\n",
        "# first 5 steps in the computation\n",
        "plt.imshow(output_GRU_over_time[:5,:])\n",
        "\n",
        "# last 5 steps in the computation\n",
        "plt.imshow(output_GRU_over_time[:5,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Yke5YXO6emI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}