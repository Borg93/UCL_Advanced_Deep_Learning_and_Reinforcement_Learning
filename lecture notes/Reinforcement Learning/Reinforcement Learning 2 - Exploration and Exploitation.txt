Exploration and Exploitation
- Exploitation: Maximize performance based on current knowledge
􏰀- Exploration: Increase knowledge
- gather enogh information to make the best overall decisions

Multi-Armed Bandit
- Set of Actions (=arms)
- At eacht step t the agent selects an Action from the set
- Env generates a reward R_t
- p(r, a) is fixed but unknown
- Goal: Maximize cumulative reward sum_i=1^t R_i

- Simple estimate for the true action value q: Average of the sampled rewards:
    - (q is the true value, whereas Q is the estimate )
    - Q_t(a) = ( sum_n=1^t R_n*I(A_n = a) )/( sum_n=1^t I(A_n=a) )
    - Incremental update: Q_t(A_t) = Q_t-1(A_t) + alpha_t (R_t - Q_t-1(A_t)) with alpha_t = 1/(N_t(A_t))

- Regret is the opportunity loss for one step: v_*-q(A_t)
    - Agent cannot observe it but we can use it to analyze different learning algorithms
- Goal: Trade-off exploration and exploitation by minimizing total regret: L_t = sum_i=1^t (v_*-q(a_i))
- Greedy policy has linear regret: In expectation, the regret grows as a function that is linear in t
- The action regret ∆a for a given action is the difference between the optimal value and the true value of a: ∆a = v∗ − q(a)

ε-Greedy Algorithm
- With probability 1 − ε select a = argmax_a∈A Q_t (a) a∈A
􏰀- With probability ε select a random action

Upper Confidence Bounds
- Select action maximizing upper confidence bound (UCB): a_t = argmax_a∈A Q_t(a)+ U_t(a)
    - Upper confidence U_t(a)
    - Small N_t(a) --> large U_t(a)
    - Large N_t(a) --> Small U_t(a)
- Hoeffding's Inequality can be used to define an optimal algorithm
    - E.g.,if R_t ∈[0,1],then p(q(a) ≥ Q_t(a)+U_t(a)) ≤ e^(−2*N_t(a)*Ut(a)^2)
        - Solve for U_t(a) and replace p = 1/t: Ut (a) = sqrt(log(t)/(2*N_t(a))
- UCB-Algorithm: a_t = argmax_a∈A Q_t(a) + c*sqrt(log(t)/(2*N_t(a))

Up to now value based algorithms, but we can also look at model based algorithms:

Bayesian Bandits:
- Bayesian bandits model parameterized distributions over rewards, p(R_t|θ, a)
     - use Beta-Discribution Beta(x_a, y_a) with x_A_t <- x_A_t +1 when R_t = 0 and y_A_t <- y_A_t +1 when R_t = 1
- with UCB:
    - Estimate upper confidence from posterior e.g., U_t(a) = cσ_t(a) where σ(a) is std dev of p_t(q(a)) 􏰀 
    - Pick action that maximizes Q_t(a) + cσ(a)
- with Probability Matching:
    - Probability matching selects action a according to probability that a is the optimal action
- with Thompson Sampling:
    - Sample Q_t(a) ∼ pt(q(a)), ∀a
    - Select action maximising sample, A_t = argmax_a∈A Q_t(a)
- Information State Space Bandits = sequential decision making:
    - At each step there is an information state s summarising all information accumulated so far (=MDP)
    - Each action a causes a transition to a new information state s (by adding information), with probability p(s'|a, s)
    - Can be solved by model free RL or Bayes-adaptive replace

Policy search
    - learning policy pi(a) directly
    - Softmax with action preferences H_t(a): 
        pi(a) = 􏰈e^( H_t(a) ) / ( sum_b e^(H_t(b) ))
    - preferences are learnable parameters and can be optimized
- Policy Gradients:
    - Updating through gradient ascent: 
        θ = θ + α∇θ E[R_t|θ], where θ are the policy parameters
        Log-likelihood trick: θ = θ + α R_t ∇θ log( π_θ(A_t) ),
    - Baselines do not change the expected update, but the do change variance

